{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.backends.mps\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pylab as pylab\n",
    "from torch.autograd.anomaly_mode import set_detect_anomaly\n",
    "\n",
    "params = {'font.size' : 16 }\n",
    "pylab.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x14b03dd30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hyperparameters\n",
    "batch_size = 10\n",
    "learning_rate = 0.003\n",
    "padding_size = 2\n",
    "filter_size = 3\n",
    "step_size = 1\n",
    "num_features = 8\n",
    "num_connected_layers = 5\n",
    "num_conv_layers = 2\n",
    "num_classifications = 10\n",
    "num_neurons_per_layer = 50\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    torch.dtype=torch.float32\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "train=True\n",
    "if train:\n",
    "    train_data = pd.read_csv(\"Data/MNIST/mnist_train.csv\")\n",
    "else:\n",
    "    train_data = pd.read_csv(\"Data/MNIST/mnist_test.csv\")\n",
    "torch.autograd.anomaly_mode.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for multi dimensioanl images with a step size of 1 - 1:1 image to conved_image size; dimensions increase by 2\n",
    "# convolution using a matrix multiplication\n",
    "class Convolver:\n",
    "    def __init__(self, kern, bias, x, device=device):\n",
    "        if(x.dim() <= 2):\n",
    "            self.img = x.unsqueeze(dim=1)\n",
    "        else:\n",
    "            self.img = x\n",
    "        self.w = kern\n",
    "        self.b = bias\n",
    "        self.device = device\n",
    "    \n",
    "    def flatten_pad(self):\n",
    "        pad_size = int((filter_size - 1) / 2) \n",
    "        img = nn.functional.pad(self.img, (pad_size, pad_size, pad_size, pad_size))\n",
    "        n = torch.zeros(2)\n",
    "        for i in range(2):\n",
    "            n[i] = ((img.shape[-i - 1] - filter_size) / step_size) + 1\n",
    "        flat_img = img.flatten().clone().view(img.shape[-4], img.shape[-3], img.shape[-2]*img.shape[-1]).float()\n",
    "        flat_kern = self.w.view(self.w.shape[-3], filter_size * filter_size)\n",
    "        return n, img, flat_img, flat_kern\n",
    "\n",
    "    def change_kern_multi(self):\n",
    "        n, img, flat_img, flat_kern = self.flatten_pad()\n",
    "        new_kern = torch.zeros(self.w.shape[-3] , int(torch.prod(n)), img.shape[-2] * img.shape[-1], device=device) #produces num_feat x n x n convolution\n",
    "        flat_kern = torch.cat([flat_kern for _ in range(1)], dim=0)\n",
    "        for k  in range(new_kern.shape[-3]):\n",
    "            start_idx = 0\n",
    "            for i in range(new_kern.shape[-2]):\n",
    "                j = 0\n",
    "                c = 0\n",
    "                if i == 0:\n",
    "                    start_idx = start_idx\n",
    "                elif i % int(n[0]) != 0:\n",
    "                    start_idx += step_size * step_size\n",
    "                else: \n",
    "                    start_idx = int((i+1-step_size)//int(n[0]) * img.shape[-2])\n",
    "                while j < new_kern.shape[-1] and start_idx+j+filter_size <= new_kern.shape[-1]:\n",
    "                    new_kern[k][i][start_idx + j:start_idx + j + filter_size] = flat_kern[k][c:c + filter_size]\n",
    "                    c += filter_size\n",
    "                    j += img.shape[-2]\n",
    "                    if c == flat_kern.shape[-1]:\n",
    "                        break\n",
    "        return n, new_kern, flat_img\n",
    "    \n",
    "    def convolve(self): #convolves image using a kernel made from self.w\n",
    "        start_time = time.time()\n",
    "        conved_final = []\n",
    "        n, changed_kern, flat_img = self.change_kern_multi()\n",
    "        to_be_cat = [flat_img for _ in range(self.img.shape[-3])]\n",
    "        copy_flat_img = torch.cat(to_be_cat, dim=1).unsqueeze(dim=-2)\n",
    "        for batch in range(self.img.shape[-4]):\n",
    "            conv = [changed_kern @ torch.transpose(copy_flat_img[batch][j], -1, -2) for j in range(self.img.shape[-3])]\n",
    "            conved_final.append(torch.concat(conv, dim=0).squeeze(dim=0))\n",
    "        conved_final = torch.stack(conved_final)\n",
    "        squeezed_convd_mat = conved_final.squeeze(dim=-1)\n",
    "        squeezed_convd_mat += self.b\n",
    "        conv_img = squeezed_convd_mat.reshape(batch_size, self.img.shape[-3] * num_features, int(n[-2]), int(n[-1]))\n",
    "        end_time = time.time()\n",
    "        return conv_img  \n",
    "    \n",
    "    def pool(self, step_size: int, conv_img: torch.tensor):\n",
    "        pool_kern = torch.ones(step_size, step_size, device=self.device)\n",
    "        pool_size = [conv_img.shape[-4], conv_img.shape[-3], int((conv_img.shape[-2] - step_size)/step_size + 1), int((conv_img.shape[-1] - step_size)/step_size + 1)]\n",
    "        pool_img = torch.zeros(pool_size, device=device)\n",
    "        for batch in range(conv_img.shape[-4]):\n",
    "            for feat in range(num_features):\n",
    "                row = 0\n",
    "                for i in range(pool_img.shape[-2]):\n",
    "                    col = 0\n",
    "                    for j in range(pool_img.shape[-1]):\n",
    "                        pixels = conv_img[batch, feat, row:row + step_size, col:col + step_size]\n",
    "                        conv = pixels * pool_kern\n",
    "                        pool_img[feat, i, j] = torch.max(conv)\n",
    "                        col += step_size \n",
    "                    row += step_size \n",
    "        return pool_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, num_neurons, num_inputs_per_neuron, device):\n",
    "        self.w = torch.randn(num_neurons, num_inputs_per_neuron, dtype=torch.float32, requires_grad=True, device=device) \n",
    "        self.b = torch.randn(num_neurons, 1, dtype=torch.float32, requires_grad=True, device=device) \n",
    "        self.gamma = torch.ones(1, dtype=torch.float32, device = device, requires_grad=True)\n",
    "        self.beta = torch.zeros(1, dtype=torch.float32, device = device, requires_grad=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = ((self.w @ x) + self.b)\n",
    "        out = torch.nn.functional.relu(out)\n",
    "        out = self.batch_norm(out)\n",
    "        return out\n",
    "    \n",
    "    def batch_norm(self, x):\n",
    "        mean = torch.mean(x, dim=0, keepdim=True)\n",
    "        variance = torch.std(x, dim=0, keepdim=True)\n",
    "        # normal = [(x[j][i] - mean) / (variance + 1e-8) for i in range(x.shape[-3])]\n",
    "        x = self.gamma * (x - mean) / (variance + 1e-8) + self.beta\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv Layer using calling convolver\n",
    "class Conv_layer():\n",
    "    def __init__(self, img, filter_size:int, num_features: int, device=device):\n",
    "        self.device = device\n",
    "        if(img.dim() <= 2):\n",
    "            self.img = img.unsqueeze(dim=0)\n",
    "        else:\n",
    "            self.img = img\n",
    "        self.num_features = num_features\n",
    "        self.w = torch.randn(num_features, filter_size, filter_size, requires_grad=True, dtype=torch.float32, device=self.device) \n",
    "        self.b = torch.randn(1, self.img.shape[-1] * self.img.shape[-2], requires_grad=True, dtype=torch.float32, device=self.device) \n",
    "        self.gamma = torch.ones(1, dtype=torch.float32, device = self.device, requires_grad=True)\n",
    "        self.beta = torch.zeros(1, dtype=torch.float32, device = self.device, requires_grad=True)\n",
    "\n",
    "    def convolve(self, image: torch.tensor):\n",
    "        convolver = Convolver(kern=self.w, x=image, bias=self.b)\n",
    "        conv_img = convolver.convolve()\n",
    "        return conv_img\n",
    "    \n",
    "    def forward_relu(self, image: torch.tensor):\n",
    "        \n",
    "        conv_img = self.convolve(image) \n",
    "        out = torch.nn.functional.relu(conv_img)\n",
    "        conv_img = self.batch_norm(image)\n",
    "        return out\n",
    "    \n",
    "    def batch_norm(self, input):\n",
    "        conv_img_temp = input.clone()\n",
    "        stack = []\n",
    "        # for j in range(conv_img_temp.shape[-4]):\n",
    "        #     mean = torch.mean(conv_img_temp[j], dim=-3, keepdim=True)\n",
    "        #     variance = torch.std(conv_img_temp[j], dim=-3, keepdim=True, unbiased=True)\n",
    "        #     normal = [(conv_img_temp[j][i] - mean) / (variance + 1e-8) for i in range(conv_img_temp.shape[-3])]\n",
    "        #     stack.append(torch.concat(normal, dim=0))\n",
    "        # conv_img_temp = self.gamma * torch.stack(stack) + self.beta\n",
    "        mean = torch.mean(conv_img_temp, dim=0, keepdim=True)\n",
    "        variance = torch.std(conv_img_temp, dim=0, keepdim=True)\n",
    "        # normal = [(conv_img_temp[j][i] - mean) / (variance + 1e-8) for i in range(conv_img_temp.shape[-3])]\n",
    "        conv_img_temp = self.gamma * (conv_img_temp - mean) / (variance + 1e-8) + self.beta\n",
    "        return conv_img_temp\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, num_conected_layers, num_conv_layers, num_neurons_per_layer, \n",
    "                 num_final_out, datagen, batch_size, device=device):\n",
    "        \n",
    "        self.device = device\n",
    "        self.datagen = datagen(batch_size)\n",
    "        img, _ = self.datagen.data_generator()\n",
    "\n",
    "        self.convlayers = [Conv_layer(img, filter_size, num_features)]\n",
    "        for _ in range(1, num_conv_layers):\n",
    "            self.convlayers.append(Conv_layer(img, filter_size, num_features))\n",
    "        self.num_inputs_per_neuron = (img.shape[-1] * img.shape[-2]) * num_features**(num_conv_layers) \n",
    "\n",
    "        self.layers = [Layer(num_neurons_per_layer, self.num_inputs_per_neuron, device=device)]\n",
    "        for _ in range(1, num_conected_layers-1):\n",
    "            self.layers.append(Layer(num_neurons_per_layer, num_neurons_per_layer, device=device))\n",
    "        self.layers.append(Layer(num_final_out, num_neurons_per_layer, device=device))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out=torch.zeros(1, requires_grad=True)\n",
    "        for i in range(0, num_conv_layers):\n",
    "            if i==0:\n",
    "                out = self.convlayers[i].forward_relu(x)\n",
    "            elif i==num_conv_layers-1:\n",
    "                out = self.convlayers[i].forward_relu(out)\n",
    "                out = out.reshape(self.num_inputs_per_neuron, batch_size)\n",
    "            else:\n",
    "                out = self.convlayers[i].forward_relu(out)\n",
    "\n",
    "\n",
    "        for i in range(0, num_connected_layers):\n",
    "            out = self.layers[i].forward(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def train(self, epochs, learning_rate):\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        for i in range(epochs):\n",
    "            x, y = self.datagen.data_generator()\n",
    "            out = self.forward(x).view(y.shape[0], y.shape[1])\n",
    "            # out = torch.tensor([-torch.inf if i.item()==0.0 else i for i in out.flatten()], device=device, requires_grad=True).view(y.shape[0], y.shape[1])\n",
    "            loss = loss_func(out, y) \n",
    "            # loss = ((y - out)**2).flatten().sum() \n",
    "            print(f\"Loss: {loss} at epoch: {i}\")\n",
    "            loss.backward()\n",
    "            for conv_layer in self.convlayers:\n",
    "                #manual clip grad, to be fixed\n",
    "                # w_grad = torch.tensor([c.item() if torch.abs(c) < 1000.0 else 1000.0 for c in conv_layer.w.grad.flatten()], dtype=torch.float, device=self.device)\n",
    "                # print(w_grad, \"\\n\", type(conv_layer.w.grad))\n",
    "                # conv_layer.w.grad = w_grad\n",
    "                # b_grad = torch.tensor([c.item() if torch.abs(c) < 100.0 else 100.0 for c in conv_layer.b.grad.flatten()], dtype=torch.float, device=self.device)\n",
    "                # conv_layer.b.grad = b_grad\n",
    "                conv_layer.w.data -= learning_rate * conv_layer.w.grad\n",
    "                conv_layer.b.data -= learning_rate * conv_layer.b.grad\n",
    "                conv_layer.w.grad = None\n",
    "                conv_layer.b.grad = None\n",
    "                \n",
    "            for layer in self.layers:\n",
    "                layer.w.data -= learning_rate * layer.w.grad\n",
    "                layer.b.data -= learning_rate * layer.b.grad\n",
    "                layer.w.grad = None\n",
    "                layer.b.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datagen:\n",
    "    def __init__(self, batch_size) -> None:\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def one_hot_encoder(self, y):\n",
    "            y_out = []\n",
    "            for i in y:\n",
    "                # num = np.array([-torch.inf for _ in range(num_classifications)])\n",
    "                num = np.zeros(num_classifications)\n",
    "                num[i] = 1\n",
    "                y_out.append(num)\n",
    "            return(torch.tensor(np.array(y_out), dtype=torch.float32)).to(device)\n",
    "    \n",
    "    def data_generator(self, fix_seed=False, train=True):\n",
    "        \n",
    "        if fix_seed==True:\n",
    "            seed_idx = torch.tensor(int(input(\"Enter seed index number\")))\n",
    "        else:\n",
    "            seed_idx = (torch.randint(low=0, high=len(train_data) - self.batch_size, size=(1,1))).item()\n",
    "        \n",
    "        y_out = torch.tensor(train_data['label'].iloc[seed_idx:seed_idx+self.batch_size].to_numpy()).to(device)\n",
    "        x = (torch.tensor(train_data.iloc[seed_idx:seed_idx+self.batch_size, 1:].to_numpy()).view(self.batch_size, 1, 28, 28).to(device)).float()\n",
    "        y_out = self.one_hot_encoder(y_out).float()\n",
    "        return x, y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = Datagen(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = datagen.data_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN(num_connected_layers, num_conv_layers, num_neurons_per_layer, num_classifications, Datagen, batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.9705283641815186 at epoch: 0\n",
      "Loss: 2.385960340499878 at epoch: 1\n",
      "Loss: 2.8118996620178223 at epoch: 2\n",
      "Loss: 2.7869930267333984 at epoch: 3\n",
      "Loss: 2.3940792083740234 at epoch: 4\n",
      "Loss: 2.946594715118408 at epoch: 5\n",
      "Loss: 2.5264711380004883 at epoch: 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/adityatandon/Documents/VS Code/Learn ML/ML_from_scratch_pytorch/cnn_mnist.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/ML_from_scratch_pytorch/cnn_mnist.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cnn\u001b[39m.\u001b[39;49mtrain(epochs\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.0005\u001b[39;49m)\n",
      "\u001b[1;32m/Users/adityatandon/Documents/VS Code/Learn ML/ML_from_scratch_pytorch/cnn_mnist.ipynb Cell 11\u001b[0m in \u001b[0;36mCNN.train\u001b[0;34m(self, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/ML_from_scratch_pytorch/cnn_mnist.ipynb#X13sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# loss = ((y - out)**2).flatten().sum() \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/ML_from_scratch_pytorch/cnn_mnist.ipynb#X13sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m at epoch: \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/ML_from_scratch_pytorch/cnn_mnist.ipynb#X13sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/ML_from_scratch_pytorch/cnn_mnist.ipynb#X13sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mfor\u001b[39;00m conv_layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvlayers:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/ML_from_scratch_pytorch/cnn_mnist.ipynb#X13sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39m#manual clip grad, to be fixed\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/ML_from_scratch_pytorch/cnn_mnist.ipynb#X13sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39m# w_grad = torch.tensor([c.item() if torch.abs(c) < 1000.0 else 1000.0 for c in conv_layer.w.grad.flatten()], dtype=torch.float, device=self.device)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/ML_from_scratch_pytorch/cnn_mnist.ipynb#X13sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39m# b_grad = torch.tensor([c.item() if torch.abs(c) < 100.0 else 100.0 for c in conv_layer.b.grad.flatten()], dtype=torch.float, device=self.device)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/ML_from_scratch_pytorch/cnn_mnist.ipynb#X13sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     \u001b[39m# conv_layer.b.grad = b_grad\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/ML_from_scratch_pytorch/cnn_mnist.ipynb#X13sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     conv_layer\u001b[39m.\u001b[39mw\u001b[39m.\u001b[39mdata \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m learning_rate \u001b[39m*\u001b[39m conv_layer\u001b[39m.\u001b[39mw\u001b[39m.\u001b[39mgrad\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cnn.train(epochs=30, learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.4011e+01, -4.7642e+01, -6.7258e+01],\n",
       "         [ 1.2936e+02, -1.0393e+01, -2.6423e+01],\n",
       "         [ 5.9078e+01, -3.3350e+00,  4.6669e+01]],\n",
       "\n",
       "        [[-1.4132e+02,  7.2798e+01,  6.5263e+01],\n",
       "         [-1.1615e+02,  7.5025e+00,  2.9697e+01],\n",
       "         [-2.3716e+01,  3.0742e+01,  2.0322e+01]],\n",
       "\n",
       "        [[ 1.1718e+00, -1.8158e+01,  5.0958e+00],\n",
       "         [-3.0504e+01, -3.9549e+01, -5.8561e+01],\n",
       "         [-1.4784e-01, -3.3997e+01, -4.1191e+01]],\n",
       "\n",
       "        [[-1.1824e+01, -7.8971e+01, -5.2081e-01],\n",
       "         [ 3.1681e+01,  3.8837e+01,  4.3595e+01],\n",
       "         [-3.3055e+01, -5.8856e+00, -3.8480e+00]],\n",
       "\n",
       "        [[-4.3322e+01, -3.5121e+01,  7.4484e+01],\n",
       "         [ 6.0489e+01,  4.8112e+00,  8.5939e+00],\n",
       "         [ 3.2113e+01,  1.4331e+01,  4.2711e+01]],\n",
       "\n",
       "        [[-1.3780e+02, -5.6488e+01, -7.4232e+01],\n",
       "         [-8.3689e+01, -9.4623e+01, -7.1628e+01],\n",
       "         [-1.4588e+01, -2.1396e+01, -6.6734e+01]],\n",
       "\n",
       "        [[ 1.8737e+01,  2.3232e+01,  2.0566e+01],\n",
       "         [ 9.4050e+01, -5.9419e+00, -4.6575e+01],\n",
       "         [-4.9353e+01, -1.1285e+02, -1.2669e+02]],\n",
       "\n",
       "        [[-3.5118e+00,  1.7694e+02,  1.0282e+02],\n",
       "         [-5.7426e+01, -8.4618e+01, -6.5104e+01],\n",
       "         [-1.2762e+02, -6.1755e+01, -7.5338e+01]]], device='mps:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.convlayers[0].w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6503,  0.9994,  0.9974, -1.0000, -1.0000,  0.3477, -0.0837, -1.0000,\n",
       "         -0.9942, -1.0000],\n",
       "        [-0.7948, -0.5337,  0.9990,  1.0000,  1.0000, -1.0000, -1.0000,  0.9999,\n",
       "          0.5209, -0.9989],\n",
       "        [-0.9187,  0.9980, -0.9490,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "         -1.0000,  1.0000],\n",
       "        [ 0.9891,  0.9569, -0.9863, -0.7765,  0.9942, -0.9389,  1.0000, -0.2122,\n",
       "         -0.9985,  1.0000],\n",
       "        [ 0.9998,  1.0000,  0.9574,  0.9695,  0.9862, -1.0000,  0.7552, -0.9999,\n",
       "         -0.9924, -0.9989],\n",
       "        [-0.9440,  0.9330, -1.0000,  0.9540, -0.9996,  1.0000, -1.0000,  1.0000,\n",
       "         -0.9981,  0.7708],\n",
       "        [ 0.9835,  0.9984, -1.0000,  0.3683,  0.9994,  0.9934,  0.9641,  1.0000,\n",
       "          0.9978, -0.9166],\n",
       "        [ 1.0000,  1.0000, -0.1194,  1.0000,  1.0000,  0.8573, -1.0000,  1.0000,\n",
       "          1.0000,  1.0000],\n",
       "        [-1.0000, -0.9999, -0.7459, -1.0000, -0.9591, -0.9800,  1.0000, -0.9865,\n",
       "         -1.0000,  1.0000],\n",
       "        [-1.0000, -1.0000,  1.0000,  0.9884, -0.9957, -0.6827,  1.0000, -1.0000,\n",
       "         -0.9997,  1.0000]], device='mps:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.forward(x=x).view(y.shape[0], y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], device='mps:0')"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
