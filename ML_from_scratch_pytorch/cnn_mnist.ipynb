{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.backends.mps\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib.cm as cm\n",
    "from torch.autograd.anomaly_mode import set_detect_anomaly\n",
    "\n",
    "params = {'font.size' : 16 }\n",
    "pylab.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x1377026d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hyperparameters\n",
    "\n",
    "padding_size = 2\n",
    "filter_size = 3\n",
    "step_size = 1\n",
    "num_features = 2\n",
    "num_connected_layers = 10\n",
    "num_conv_layers = 3\n",
    "num_classifications = 10\n",
    "num_neurons_per_layer = 50\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    torch.dtype=torch.float32\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "torch.autograd.anomaly_mode.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"/Users/adityatandon/Documents/VS Code/Learn ML/Data/MNIST/mnist_train.csv\")\n",
    "test_data = pd.read_csv(\"/Users/adityatandon/Documents/VS Code/Learn ML/Data/MNIST/mnist_test.csv\")\n",
    "test_img = torch.tensor(train_data.iloc[0].to_list()[1:], dtype=torch.float32, device=device).view(28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_img.to(\"cpu\"), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv Layer using normal for loops\n",
    "class Conv_layer2():\n",
    "    def __init__(self, filter_size:int, num_features: int, device=device):\n",
    "        self.device = device\n",
    "        self.num_features = num_features\n",
    "        self.kernel = torch.randn(num_features, filter_size, filter_size, dtype=torch.float32, device=self.device)\n",
    "        # self.kernel = torch.tensor([[[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]], [[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]], dtype=torch.float32, device=device)\n",
    "        # # print(padded_image.shape, self.kernel.shape)\n",
    "        self.bias = torch.randn(filter_size, filter_size, dtype=torch.float32, device=self.device)\n",
    "        print(self.kernel)\n",
    "\n",
    "    def convolve(self, image: torch.tensor):\n",
    "        # self.kernel = torch.flip(self.kernel, dims=(0,))\n",
    "        start_time = time.time()\n",
    "        if(image.dim() <= 2):\n",
    "            unsqueezed_img = image.unsqueeze(dim=0)\n",
    "        else:\n",
    "            unsqueezed_img = image\n",
    "        padded_image = nn.functional.pad(unsqueezed_img, (filter_size - 1, filter_size - 1, filter_size - 1, filter_size - 1, 0, 0))\n",
    "        # print(padded_image.shape, padded_image[0, :10, :10])\n",
    "        conv_img = torch.zeros(size=(num_features*unsqueezed_img.shape[0], unsqueezed_img.shape[1], unsqueezed_img.shape[2]), device=self.device)\n",
    "        for feat in range(num_features):\n",
    "            for k in range(padded_image.shape[0]):\n",
    "                for i in range(padded_image.shape[1] - self.kernel[feat].shape[0]):\n",
    "                    for j in range(padded_image.shape[2] - self.kernel[feat].shape[1]):\n",
    "                        pixels = padded_image[k, i:i + self.kernel[feat].shape[0], j:j + self.kernel[feat].shape[1]]\n",
    "                        conv = pixels * self.kernel[feat]\n",
    "                        conv += self.bias\n",
    "                        conv_img[feat, i:i + self.kernel[feat].shape[0] - step_size, j:j + self.kernel[feat].shape[1] - step_size] += torch.sum(conv)\n",
    "                        # conv_img[feat, i:i + 1, j:j + 1] += torch.sum(conv)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(end_time - start_time)\n",
    "        return conv_img, self.kernel\n",
    "    \n",
    "    def pool(self, step_size: int, conv_img: torch.tensor):\n",
    "        pool_kern = torch.ones(step_size, step_size, device=self.device)\n",
    "        pool_size = [conv_img.shape[-3], int((conv_img.shape[-2] - step_size)/step_size + 1), int((conv_img.shape[-1] - step_size)/step_size + 1)]\n",
    "        pool_img = torch.zeros(pool_size, device=device)\n",
    "        for feat in range(num_features):\n",
    "            row = 0\n",
    "            for i in range(pool_img.shape[-2]):\n",
    "                col = 0\n",
    "                for j in range(pool_img.shape[-1]):\n",
    "                    pixels = conv_img[feat, row:row + step_size, col:col + step_size]\n",
    "                    conv = pixels * pool_kern\n",
    "                    pool_img[feat, i, j] = torch.max(conv)\n",
    "                    col += step_size \n",
    "                    \n",
    "                row += step_size \n",
    "        return pool_img\n",
    "    \n",
    "    def forward(self, image: torch.tensor, step_size: int):\n",
    "        conv_img, kernel = self.convolve(image)\n",
    "        out = conv_img.relu()\n",
    "        return out\n",
    "    \n",
    "    # def train(self, x, )\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = Conv_layer2(filter_size=filter_size, num_features=num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_img, kernel = conv_layer.convolve(image=test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_img = conv_layer.pool(step_size=step_size, conv_img=conv_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10,10))\n",
    "ax[0].imshow(conv_img[0].to(\"cpu\"), cmap='gray')\n",
    "ax[1].imshow(pool_img[0].to(\"cpu\"), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = conv_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern = torch.randn(num_features, filter_size, filter_size, requires_grad=True, dtype=torch.float32, device=device)\n",
    "kern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for multi dimensioanl images with a step size of 1 - 1:1 image to conved_image size; dimensions increase by 2\n",
    "# convolution using a matrix multiplication\n",
    "class Convolver:\n",
    "    \n",
    "    def __init__(self, kern, bias, x, device=device) -> None:\n",
    "        \n",
    "        if(x.dim() <= 2):\n",
    "            self.img = x.unsqueeze(dim=1)\n",
    "        else:\n",
    "            self.img = x\n",
    "        # print(self.img.shape)\n",
    "        # self.w = torch.randn(num_features * self.img.shape[-3], filter_size, filter_size, requires_grad=True, dtype=torch.float32, device=device)\n",
    "        # self.b = torch.randn(self.img.shape[-1] * self.img.shape[-2], requires_grad=True, dtype=torch.float32, device=device)\n",
    "        self.w = kern\n",
    "        # print(self.w.shape)\n",
    "        self.b = bias\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def flatten_pad(self):\n",
    "        \n",
    "        pad_size = int((filter_size - 1) / 2) \n",
    "        img = nn.functional.pad(self.img, (pad_size, pad_size, pad_size, pad_size))\n",
    "        # print(img.shape)\n",
    "        n = torch.zeros(2)\n",
    "        for i in range(2):\n",
    "            n[i] = ((img.shape[-i - 1] - filter_size) / step_size) + 1\n",
    "        # print(n)\n",
    "        # flat_img = img.flatten().clone().detach().view(img.shape[-3], img.shape[-2]*img.shape[-1]).float()\n",
    "        flat_img = img.flatten().clone().view(img.shape[-4], img.shape[-3], img.shape[-2]*img.shape[-1]).float()\n",
    "        # flat_kern = self.w.view(num_features * self.img.shape[-3], filter_size * filter_size)\n",
    "        # print(\"img shape = \",self.img.shape)\n",
    "        flat_kern = self.w.view(self.w.shape[-3], filter_size * filter_size)\n",
    "        # print(\"flat kern shape = \", flat_kern.shape)\n",
    "        return n, img, flat_img, flat_kern\n",
    "    \n",
    "\n",
    "    def change_kern_multi(self):\n",
    "\n",
    "        n, img, flat_img, flat_kern = self.flatten_pad()\n",
    "\n",
    "        new_kern = torch.zeros(self.w.shape[-3] , int(torch.prod(n)), img.shape[-2] * img.shape[-1], device=device) #produces num_feat x n x n convolution\n",
    "        # print(\"changed kern shape = \", new_kern.shape)\n",
    "        flat_kern = torch.cat([flat_kern for _ in range(1)], dim=0)\n",
    "        # print(\"flat kern_cat shape = \", flat_kern.shape)\n",
    "\n",
    "        for k  in range(new_kern.shape[-3]):\n",
    "            start_idx = 0\n",
    "            for i in range(new_kern.shape[-2]):\n",
    "                j = 0\n",
    "                c = 0\n",
    "                if i == 0:\n",
    "                    start_idx = start_idx\n",
    "                elif i % int(n[0]) != 0:\n",
    "                    start_idx += step_size * step_size\n",
    "                else: \n",
    "                    start_idx = int((i+1-step_size)//int(n[0]) * img.shape[-2])\n",
    "\n",
    "                while j < new_kern.shape[-1] and start_idx+j+filter_size <= new_kern.shape[-1]:\n",
    "                    new_kern[k][i][start_idx + j:start_idx + j + filter_size] = flat_kern[k][c:c + filter_size]\n",
    "                    c += filter_size\n",
    "                    j += img.shape[-2]\n",
    "                    if c == flat_kern.shape[-1]:\n",
    "                        break\n",
    "        return n, new_kern, flat_img\n",
    "    \n",
    "    def convolve(self): #convolves image using a kernel made from self.w\n",
    "        start_time = time.time()\n",
    "        # conved_final = torch.empty(batch_size, num_features*self.img.shape[-3], self.img.shape[-2], self.img.shape[-1]) \n",
    "        # for i in range(batch_size):\n",
    "        conved_final = []\n",
    "        n, changed_kern, flat_img = self.change_kern_multi()\n",
    "        # print(\"flat img shape = \", flat_img.shape)\n",
    "        to_be_cat = [flat_img for _ in range(self.img.shape[-3])]\n",
    "        copy_flat_img = torch.cat(to_be_cat, dim=1).unsqueeze(dim=-2)\n",
    "        # print(\"copy flat img shape\",copy_flat_img.shape)\n",
    "        # print(changed_kern.shape, \"changed_kern\")\n",
    "        # copy_flat_img = flat_img\n",
    "        # conved_final = changed_kern @ copy_flat_img[:][0].to(self.device)\n",
    "        for batch in range(self.img.shape[-4]):\n",
    "            # torch.stack((conved_final,(changed_kern @ copy_flat_img[batch][i]).unsqueeze(dim=-1).to(self.device)), dim=1)\n",
    "            conv = [changed_kern @ torch.transpose(copy_flat_img[batch][j], -1, -2) for j in range(self.img.shape[-3])]\n",
    "            conved_final.append(torch.concat(conv, dim=0).squeeze(dim=0))\n",
    "        conved_final = torch.stack(conved_final)\n",
    "        # print(conved_final.shape)\n",
    "        \n",
    "        squeezed_convd_mat = conved_final.squeeze(dim=-1)\n",
    "        squeezed_convd_mat += self.b\n",
    "        # print(squeezed_convd_mat.shape)\n",
    "        \n",
    "        conv_img = squeezed_convd_mat.reshape(batch_size, self.img.shape[-3] * num_features, int(n[-2]), int(n[-1]))\n",
    "\n",
    "        end_time = time.time()\n",
    "        # print(end_time - start_time)\n",
    "        return conv_img #returns convolved image \n",
    "    \n",
    "    def pool(self, step_size: int, conv_img: torch.tensor):\n",
    "        pool_kern = torch.ones(step_size, step_size, device=self.device)\n",
    "        pool_size = [conv_img.shape[-4], conv_img.shape[-3], int((conv_img.shape[-2] - step_size)/step_size + 1), int((conv_img.shape[-1] - step_size)/step_size + 1)]\n",
    "        pool_img = torch.zeros(pool_size, device=device)\n",
    "        for batch in range(conv_img.shape[-4]):\n",
    "            for feat in range(num_features):\n",
    "                row = 0\n",
    "                for i in range(pool_img.shape[-2]):\n",
    "                    col = 0\n",
    "                    for j in range(pool_img.shape[-1]):\n",
    "                        pixels = conv_img[batch, feat, row:row + step_size, col:col + step_size]\n",
    "                        conv = pixels * pool_kern\n",
    "                        pool_img[feat, i, j] = torch.max(conv)\n",
    "                        col += step_size \n",
    "                    row += step_size \n",
    "        return pool_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_bias = torch.randn(1, 28*28, requires_grad=True, dtype=torch.float32, device=device)\n",
    "rand_kern = torch.randn(num_features, filter_size, filter_size, requires_grad=True, dtype=torch.float32, device=device)\n",
    "convolver = Convolver(kern=rand_kern, bias=rand_bias, x=x.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 28, 28])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conved = convolver.convolve()\n",
    "conved.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x1440 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,conved.shape[1], figsize=(20,20))\n",
    "for i in range(conved.shape[1]):\n",
    "    ax[i].imshow(conved[0][i].to(\"cpu\").detach().numpy(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, num_neurons, num_inputs_per_neuron, device):\n",
    "        self.w = torch.randn(num_neurons, num_inputs_per_neuron, dtype=torch.float32, requires_grad=True, device=device) \n",
    "        self.b = torch.randn(num_neurons, 1, dtype=torch.float32, requires_grad=True, device=device) \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        outer = ((self.w @ x) + self.b)\n",
    "        # for i in range(outer.shape[-2]):\n",
    "        #     mean = torch.mean(outer[i, :]).to(device)\n",
    "        #     sd = torch.sqrt(torch.var(outer[i, :])).to(device)\n",
    "        #     outer[i, :] = (outer[i, :] - mean) / sd\n",
    "        out = torch.nn.functional.tanh(outer)\n",
    "        # print(out.shape)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6789,  0.4792,  0.6931, -0.6475],\n",
       "         [ 0.3496,  0.8261, -0.6828, -0.8443],\n",
       "         [ 1.6337, -0.0258,  1.3919, -0.9389]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ten = torch.randn(1,2,3,4)\n",
    "test_ten[:,0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-6.7825e-01,  3.1319e-01,  1.7143e+01, -6.0761e-01],\n",
       "           [ 8.7490e+01,  3.7891e-01, -1.4079e+00, -2.3284e+00],\n",
       "           [ 8.8929e-01, -6.9566e-02,  1.5702e+00, -5.2097e-01]]]]),\n",
       " tensor([[[[1.0010, 1.5301, 0.0404, 1.0656],\n",
       "           [0.0040, 2.1803, 0.4850, 0.3626],\n",
       "           [1.8371, 0.3708, 0.8865, 1.8023]]]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance = torch.var(test_ten, dim=-3, keepdim=True)\n",
    "check = test_ten[:, 0,:,:] / variance\n",
    "check, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv Layer using calling convolver\n",
    "\n",
    "# commented out w worked\n",
    "\n",
    "class Conv_layer():\n",
    "    def __init__(self, img, filter_size:int, num_features: int, device=device):\n",
    "        self.device = device\n",
    "        if(img.dim() <= 2):\n",
    "            self.img = img.unsqueeze(dim=0)\n",
    "        else:\n",
    "            self.img = img\n",
    "        self.num_features = num_features\n",
    "        # self.w = torch.randn(num_features * self.img.shape[-3], filter_size, filter_size, requires_grad=True, dtype=torch.float32, device=self.device)\n",
    "        self.w = torch.randn(num_features, filter_size, filter_size, requires_grad=True, dtype=torch.float32, device=self.device) \n",
    "        # self.wel = torch.tensor([[[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]], [[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]], dtype=torch.float32, device=device)\n",
    "        # # print(padded_image.shape, self.wel.shape)\n",
    "        self.b = torch.randn(1, self.img.shape[-1] * self.img.shape[-2], requires_grad=True, dtype=torch.float32, device=self.device) \n",
    "        self.gamma = torch.randn(1, dtype=torch.float32, device = self.device, requires_grad=True)\n",
    "        self.beta = torch.randn(1, dtype=torch.float32, device = self.device, requires_grad=True)\n",
    "\n",
    "    def convolve(self, image: torch.tensor):\n",
    "        # print(\"kern shape = \", self.w.shape)\n",
    "        convolver = Convolver(kern=self.w, x=image, bias=self.b)\n",
    "        conv_img = convolver.convolve()\n",
    "        return conv_img\n",
    "    \n",
    "    def forward_relu(self, image: torch.tensor):\n",
    "        conv_img = self.convolve(image) \n",
    "        #batch_norm\n",
    "        normalised = self.batch_norm(conv_img)\n",
    "        conv_img = normalised \n",
    "        # conv_img_temp = conv_img.clone() \n",
    "        # for i in range(conv_img_temp.shape[-4]):\n",
    "        #     for j in range(conv_img_temp.shape[-3]):\n",
    "        #         mean = torch.mean(conv_img_temp[i,j,:,:])\n",
    "        #         # variance=torch.zeros(1, requires_grad=True)\n",
    "        #         variance = torch.var(conv_img_temp, dim=-3, keepdim=True)\n",
    "        #         copy = conv_img_temp[i,j,:,:].clone()\n",
    "        #         conv_img_temp[i,j,:,:] = (copy - mean) / torch.sqrt(variance)\n",
    "        out = torch.nn.functional.tanh(conv_img)\n",
    "        return out\n",
    "    \n",
    "    def batch_norm(self, input):\n",
    "        # conv_img_temp = input.clone()\n",
    "        # for i in range(conv_img_temp.shape[-3]):\n",
    "        #     mean = torch.mean(conv_img_temp, dim=-3, keepdim=True)\n",
    "        #     # variance=torch.zeros(1, requires_grad=True)\n",
    "        #     variance = torch.var(conv_img_temp, dim=-3, keepdim=True)\n",
    "        #     normalised = (conv_img_temp[:,i,:,:] - mean) / torch.sqrt(variance + 1e-8)\n",
    "        #     # print(\"conv: \", conv_img_temp.shape)\n",
    "        #     # print(\"mean: \", mean.shape)\n",
    "        #     # print(\"var: \", var.shape)\n",
    "        #     conv_img_temp[:,i,:,:] = normalised\n",
    "        conv_img_temp = input.clone()\n",
    "        \n",
    "        stack = []\n",
    "        for j in range(conv_img_temp.shape[-4]):\n",
    "            # for i in range(conv_img_temp.shape[-3]):  # Iterate over the channel dimension\n",
    "            mean = torch.mean(conv_img_temp, dim=-3, keepdim=True)\n",
    "            variance = torch.var(conv_img_temp, dim=-3, keepdim=True, unbiased=True)\n",
    "            normal = [(conv_img_temp[j][i] - mean) / torch.sqrt(variance + 1e-8) for i in range(conv_img_temp.shape[-3])]\n",
    "            print(normal[0].shape)\n",
    "            stack.append(torch.concat(normal, dim=0))\n",
    "    \n",
    "        conv_img_temp = torch.stack(stack)\n",
    "\n",
    "        return conv_img_temp\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pool_Layer:\n",
    "\n",
    "    def __init__(self, device=device):\n",
    "        self.device=device\n",
    "        pass\n",
    "\n",
    "    def pool(self, step_size: int, conv_img: torch.tensor):\n",
    "        pool_kern = torch.ones(step_size, step_size, device=self.device)\n",
    "        pool_size = [batch_size, conv_img.shape[-3], int((conv_img.shape[-2] - step_size)/step_size + 1), int((conv_img.shape[-1] - step_size)/step_size + 1)]\n",
    "        pool_img = torch.zeros(pool_size, device=device)\n",
    "        for feat in range(num_features):\n",
    "            row = 0\n",
    "            for i in range(pool_img.shape[-2]):\n",
    "                col = 0\n",
    "                for j in range(pool_img.shape[-1]):\n",
    "                    pixels = conv_img[:, feat, row:row + step_size, col:col + step_size]\n",
    "                    conv = pixels * pool_kern\n",
    "                    pool_img[:, feat, i, j] = torch.max(conv)\n",
    "                    col += step_size \n",
    "                    \n",
    "                row += step_size \n",
    "        return pool_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    #num_inputs per neuron=batch_size\n",
    "    def __init__(self, num_conected_layers, num_conv_layers, num_neurons_per_layer, \n",
    "                 num_final_out, datagen, batch_size, device=device):\n",
    "        \n",
    "        self.device = device\n",
    "        self.datagen = datagen(batch_size)\n",
    "        img, _ = self.datagen.data_generator()\n",
    "        print(img.shape)\n",
    "        self.convlayers = [Conv_layer(img, filter_size, num_features)]\n",
    "        for _ in range(1, num_conv_layers):\n",
    "            self.convlayers.append(Conv_layer(img, filter_size, num_features))\n",
    "\n",
    "        self.num_inputs_per_neuron = (img.shape[-1] * img.shape[-2]) * num_features**(num_conv_layers) \n",
    "        # print(\"num_inputs = \", self.num_inputs_per_neuron) \n",
    "\n",
    "        self.layers = [Layer(num_neurons_per_layer, self.num_inputs_per_neuron, device=device)]\n",
    "        for _ in range(1, num_conected_layers-1):\n",
    "            self.layers.append(Layer(num_neurons_per_layer, num_neurons_per_layer, device=device))\n",
    "        self.layers.append(Layer(num_final_out, num_neurons_per_layer, device=device))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        c = 0\n",
    "        out=torch.zeros(1, requires_grad=True)\n",
    "        for i in range(0, num_conv_layers):\n",
    "            if i==0:\n",
    "                out = self.convlayers[0].forward_relu(x)\n",
    "            elif i==num_conv_layers-1:\n",
    "                out = self.convlayers[i].convolve(out)\n",
    "                out = out.reshape(self.num_inputs_per_neuron, batch_size)\n",
    "            else:\n",
    "                out = self.convlayers[i].forward_relu(out)\n",
    "\n",
    "        for i in range(0, num_connected_layers):\n",
    "            # print(\"out shape check= \",out.shape)\n",
    "            out = self.layers[i].forward(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def train(self, epochs, learning_rate):\n",
    "\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        for i in range(epochs):\n",
    "            x, y = self.datagen.data_generator()\n",
    "            out = self.forward(x).view(y.shape[0], y.shape[1])\n",
    "            # out = torch.tensor([-torch.inf if i.item()==0.0 else i for i in out.flatten()], device=device, requires_grad=True).view(y.shape[0], y.shape[1])\n",
    "            loss = loss_func(out, y) \n",
    "            # loss = ((y - out)**2).flatten().sum() \n",
    "            print(f\"Loss: {loss} at epoch: {i}\")\n",
    "            loss.backward()\n",
    "\n",
    "            for conv_layer in self.convlayers:\n",
    "                # w_grad = torch.tensor([c.item() if torch.abs(c) < 1000.0 else 1000.0 for c in conv_layer.w.grad.flatten()], dtype=torch.float, device=self.device)\n",
    "                # print(w_grad, \"\\n\", type(conv_layer.w.grad))\n",
    "                # conv_layer.w.grad = w_grad\n",
    "                # b_grad = torch.tensor([c.item() if torch.abs(c) < 100.0 else 100.0 for c in conv_layer.b.grad.flatten()], dtype=torch.float, device=self.device)\n",
    "                # conv_layer.b.grad = b_grad\n",
    "                conv_layer.w.data -= learning_rate * conv_layer.w.grad\n",
    "                conv_layer.b.data -= learning_rate * conv_layer.b.grad\n",
    "                conv_layer.w.grad = None\n",
    "                conv_layer.b.grad = None\n",
    "                \n",
    "            for layer in self.layers:\n",
    "                layer.w.data -= learning_rate * layer.w.grad\n",
    "                layer.b.data -= learning_rate * layer.b.grad\n",
    "                layer.w.grad = None\n",
    "                layer.b.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "emb_dim = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datagen:\n",
    "    def __init__(self, batch_size) -> None:\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def data_generator(self, fix_seed=False, train=True):\n",
    "        if fix_seed==True:\n",
    "            seed_idx = torch.tensor(int(input(\"Enter seed index number\")))\n",
    "        else:\n",
    "            seed_idx = (torch.randint(low=0, high=len(train_data) - self.batch_size, size=(1,1))).item()\n",
    "        \n",
    "        if train:\n",
    "            y_out = torch.tensor(train_data['label'].iloc[seed_idx:seed_idx+self.batch_size].to_numpy()).to(device)\n",
    "            x = (torch.tensor(train_data.iloc[seed_idx:seed_idx+self.batch_size, 1:].to_numpy()).view(self.batch_size, 1, 28, 28).to(device))\n",
    "        else:\n",
    "            y_out = torch.tensor(test_data['label'].iloc[seed_idx:seed_idx+self.batch_size].to_numpy()).to(device)\n",
    "            x = (torch.tensor(test_data.iloc[seed_idx:seed_idx+self.batch_size, 1:].to_numpy()).view(batch_size, 1, 28, 28).to(device))\n",
    "            \n",
    "\n",
    "        def one_hot_encoder(y):\n",
    "            y_out = []\n",
    "            for i in y:\n",
    "                # num = np.array([-torch.inf for _ in range(num_classifications)])\n",
    "                num = np.zeros(num_classifications)\n",
    "                num[i] = 1\n",
    "                y_out.append(num)\n",
    "            return(torch.tensor(np.array(y_out), dtype=torch.float32)).to(device)\n",
    "\n",
    "        embedding = nn.Embedding(num_classifications, emb_dim, device=device)\n",
    "        # y_out = torch.transpose(embedding(y_out), 0, 1)\n",
    "        y_out = one_hot_encoder(y_out)\n",
    "        return x, y_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = Datagen(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28]) tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "x, y = datagen.data_generator()\n",
    "print(x.shape, y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN(num_connected_layers, num_conv_layers, num_neurons_per_layer, num_classifications, Datagen, batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 2, 28, 28]' is invalid for input of size 3136",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/adityatandon/Documents/VS Code/Learn ML/Tests/CNN_test.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X66sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cnn\u001b[39m.\u001b[39;49mforward(x\u001b[39m=\u001b[39;49mx)\u001b[39m.\u001b[39mview(y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n",
      "\u001b[1;32m/Users/adityatandon/Documents/VS Code/Learn ML/Tests/CNN_test.ipynb Cell 30\u001b[0m in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X66sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mreshape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_inputs_per_neuron, batch_size)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X66sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X66sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m         out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvlayers[i]\u001b[39m.\u001b[39;49mforward_relu(out)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X66sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, num_connected_layers):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X66sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39m# print(\"out shape check= \",out.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X66sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[i]\u001b[39m.\u001b[39mforward(out)\n",
      "\u001b[1;32m/Users/adityatandon/Documents/VS Code/Learn ML/Tests/CNN_test.ipynb Cell 30\u001b[0m in \u001b[0;36mConv_layer.forward_relu\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X66sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_relu\u001b[39m(\u001b[39mself\u001b[39m, image: torch\u001b[39m.\u001b[39mtensor):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X66sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     conv_img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvolve(image) \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X66sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39m#batch_norm\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X66sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     normalised \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_norm(conv_img)\n",
      "\u001b[1;32m/Users/adityatandon/Documents/VS Code/Learn ML/Tests/CNN_test.ipynb Cell 30\u001b[0m in \u001b[0;36mConv_layer.convolve\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X66sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvolve\u001b[39m(\u001b[39mself\u001b[39m, image: torch\u001b[39m.\u001b[39mtensor):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X66sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# print(\"kern shape = \", self.w.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X66sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     convolver \u001b[39m=\u001b[39m Convolver(kern\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw, x\u001b[39m=\u001b[39mimage, bias\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X66sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     conv_img \u001b[39m=\u001b[39m convolver\u001b[39m.\u001b[39;49mconvolve()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X66sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m conv_img\n",
      "\u001b[1;32m/Users/adityatandon/Documents/VS Code/Learn ML/Tests/CNN_test.ipynb Cell 30\u001b[0m in \u001b[0;36mConvolver.convolve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X66sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m squeezed_convd_mat \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X66sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m \u001b[39m# print(squeezed_convd_mat.shape)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X66sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m conv_img \u001b[39m=\u001b[39m squeezed_convd_mat\u001b[39m.\u001b[39;49mreshape(batch_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimg\u001b[39m.\u001b[39;49mshape[\u001b[39m-\u001b[39;49m\u001b[39m3\u001b[39;49m] \u001b[39m*\u001b[39;49m num_features, \u001b[39mint\u001b[39;49m(n[\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m]), \u001b[39mint\u001b[39;49m(n[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X66sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X66sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39m# print(end_time - start_time)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 2, 28, 28]' is invalid for input of size 3136"
     ]
    }
   ],
   "source": [
    "cnn.forward(x=x).view(y.shape[0], y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.868053913116455 at epoch: 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [MPSFloatType [1, 4, 28, 28]], which is output 0 of torch::autograd::CopySlices, is at version 4; expected version 3 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/adityatandon/Documents/VS Code/Learn ML/Tests/CNN_test.ipynb Cell 32\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X65sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cnn\u001b[39m.\u001b[39;49mtrain(epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.003\u001b[39;49m)\n",
      "\u001b[1;32m/Users/adityatandon/Documents/VS Code/Learn ML/Tests/CNN_test.ipynb Cell 32\u001b[0m in \u001b[0;36mCNN.train\u001b[0;34m(self, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X65sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m# loss = ((y - out)**2).flatten().sum() \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X65sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m at epoch: \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X65sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X65sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mfor\u001b[39;00m conv_layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvlayers:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X65sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     \u001b[39m# w_grad = torch.tensor([c.item() if torch.abs(c) < 1000.0 else 1000.0 for c in conv_layer.w.grad.flatten()], dtype=torch.float, device=self.device)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X65sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     \u001b[39m# print(w_grad, \"\\n\", type(conv_layer.w.grad))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X65sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39m# conv_layer.w.grad = w_grad\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X65sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     \u001b[39m# b_grad = torch.tensor([c.item() if torch.abs(c) < 100.0 else 100.0 for c in conv_layer.b.grad.flatten()], dtype=torch.float, device=self.device)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X65sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39m# conv_layer.b.grad = b_grad\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Learn%20ML/Tests/CNN_test.ipynb#X65sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     conv_layer\u001b[39m.\u001b[39mw\u001b[39m.\u001b[39mdata \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m learning_rate \u001b[39m*\u001b[39m conv_layer\u001b[39m.\u001b[39mw\u001b[39m.\u001b[39mgrad\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [MPSFloatType [1, 4, 28, 28]], which is output 0 of torch::autograd::CopySlices, is at version 4; expected version 3 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "cnn.train(epochs=200, learning_rate=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-0.5986,  0.6070],\n",
       "           [-0.6288,  4.8591],\n",
       "           [ 0.6652, -0.9033]],\n",
       " \n",
       "          [[ 8.8204, -1.6444],\n",
       "           [-0.2468, 10.5560],\n",
       "           [-0.1618, -0.9028]]],\n",
       " \n",
       " \n",
       "         [[[-0.0774,  0.5193],\n",
       "           [-5.6927,  0.8019],\n",
       "           [ 2.5390, -0.7013]],\n",
       " \n",
       "          [[ 1.1399, -1.4069],\n",
       "           [-2.2342,  1.7421],\n",
       "           [-0.6177, -0.7008]]]]),\n",
       " tensor([[[[-0.1104,  1.0044],\n",
       "           [-2.7692,  0.5621],\n",
       "           [ 1.0932, -0.7909]],\n",
       " \n",
       "          [[-0.0258, -0.3340],\n",
       "           [-1.1425,  0.4769],\n",
       "           [-1.4092,  1.0791]],\n",
       " \n",
       "          [[ 0.6723, -1.5676],\n",
       "           [ 1.3950,  1.1040],\n",
       "           [-0.6417,  0.2060]]],\n",
       " \n",
       " \n",
       "         [[[ 1.6274, -2.7210],\n",
       "           [-1.0868,  1.2211],\n",
       "           [-0.2660, -0.7905]],\n",
       " \n",
       "          [[ 1.9373,  0.0235],\n",
       "           [ 0.2744, -0.1599],\n",
       "           [ 0.0672,  1.1450]],\n",
       " \n",
       "          [[-0.2697, -1.7399],\n",
       "           [-0.6700, -0.2894],\n",
       "           [-1.1986, -0.5805]]]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ten[:, 0,:,:] / torch.var(test_ten, dim=-3, keepdim=True), test_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8211, -0.4645,  0.6627],\n",
      "         [-0.2683, -0.2269, -0.4489],\n",
      "         [ 1.6862,  0.5582, -0.9445]],\n",
      "\n",
      "        [[ 0.4805, -1.4488, -1.0407],\n",
      "         [-1.7504, -2.1450, -0.4874],\n",
      "         [-1.4110, -1.5698, -1.9108]],\n",
      "\n",
      "        [[-0.0635,  1.1047,  0.5695],\n",
      "         [-1.0960,  1.0888,  2.2447],\n",
      "         [-0.3512, -0.7286, -0.2892]],\n",
      "\n",
      "        [[-0.8823, -1.3006,  0.3239],\n",
      "         [-0.2726, -0.5126, -1.5790],\n",
      "         [-1.1502, -1.4733, -2.0099]]], device='mps:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(cnn.convlayers[1].w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pool_layer:\n",
    "    def __init__(self, x, device=device):\n",
    "        self.device = device\n",
    "        pass\n",
    "\n",
    "    def pool(self, step_size: int, conv_img: torch.tensor):\n",
    "        pool_kern = torch.ones(step_size, step_size, device=self.device)\n",
    "        pool_size = [conv_img.shape[-3], int((conv_img.shape[-2] - step_size)/step_size + 1), int((conv_img.shape[-1] - step_size)/step_size + 1)]\n",
    "        pool_img = torch.zeros(pool_size, device=device)\n",
    "        for feat in range(num_features):\n",
    "            row = 0\n",
    "            for i in range(pool_img.shape[-2]):\n",
    "                col = 0\n",
    "                for j in range(pool_img.shape[-1]):\n",
    "                    pixels = conv_img[feat, row:row + step_size, col:col + step_size]\n",
    "                    conv = pixels * pool_kern\n",
    "                    pool_img[feat, i, j] = torch.max(conv)\n",
    "                    col += step_size \n",
    "                    \n",
    "                row += step_size \n",
    "        return pool_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for multi dimensioanl images, conved img size != img size, reduces size by 2 but increases dims by 2\n",
    "# convolution using a toeplitz matrix\n",
    "class Convolver2:\n",
    "    \n",
    "    def __init__(self, kernel, x, bias, device=device) -> None:\n",
    "        self.kern = torch.randn(num_features, filter_size, filter_size, requires_grad=True, dtype=torch.float32, device=device)\n",
    "        print(\"Kern = \\n\", self.kern)\n",
    "        if(x.dim() <= 2):\n",
    "            self.img = x.unsqueeze(dim=0)\n",
    "        else:\n",
    "            self.img = x\n",
    "\n",
    "    def find_n(self, i, n):\n",
    "        if n >= 0 :\n",
    "            end_pos = self.kern.shape[i] - 1 + ((n-1) * step_size) \n",
    "            if end_pos >= (self.img.shape[i] - 1):\n",
    "                return n\n",
    "            else:\n",
    "                return self.find_n(i, n+1)\n",
    "        else:\n",
    "            print(\"Initialise n above or equal to 0\") \n",
    "            return -9999\n",
    "\n",
    "\n",
    "    def get_n_flatten_pad(self):\n",
    "        n = torch.zeros(2)\n",
    "        for i in range(n.shape[0]):\n",
    "            n[i] = self.find_n(-i - 1, n[i])\n",
    "            \n",
    "        self.bias = torch.randn(int(torch.prod(n)), requires_grad=True, dtype=torch.float32, device=device)\n",
    "        test_pad_0 = filter_size + (int(n[-1].item()) - 1) * step_size - self.img.shape[-1]\n",
    "        test_pad_1 = filter_size + (int(n[-2].item()) - 1) * step_size - self.img.shape[-2]\n",
    "        print(test_pad_0)\n",
    "        pad_black = nn.functional.pad(self.img, (0, test_pad_0, 0, test_pad_1))\n",
    "        # pad_black = nn.functional.pad(self.img, (1, 1, 1, 1))\n",
    "        # n = torch.zeros(2)\n",
    "        # for i in range(2):\n",
    "        #     n[i] = ((pad_black.shape[-i - 1] - filter_size) / step_size) + 1\n",
    "        \n",
    "        flat_pad_black = pad_black.flatten().clone().detach().view(pad_black.shape[-3], pad_black.shape[-2]*pad_black.shape[-1]).float()\n",
    "\n",
    "        flat_kern = self.kern.view(num_features, filter_size * filter_size)\n",
    "\n",
    "        return n, pad_black, flat_pad_black, flat_kern, test_pad_0, test_pad_1\n",
    "    \n",
    "\n",
    "    def change_kern_multi(self):\n",
    "        n, pad_black, flat_pad_black, flat_kern, test_pad_0, test_pad_1 = self.get_n_flatten_pad()\n",
    "\n",
    "        new_kern = torch.zeros(num_features * self.img.shape[-3], int(torch.prod(n).item()), pad_black.shape[-2] * pad_black.shape[-1], device=device) #produces num_feat x n x n convolution\n",
    "        print(new_kern.shape)\n",
    "\n",
    "        flat_kern = torch.cat([flat_kern, flat_kern], dim=0)\n",
    "        print(flat_kern.shape)\n",
    "        print(\"n = \", n)\n",
    "        print(\"Pad_ =\", pad_black.shape)\n",
    "\n",
    "        for k  in range(new_kern.shape[-3]):\n",
    "\n",
    "            num_steps = 0\n",
    "            start_idx = 0\n",
    "            for i in range(new_kern.shape[-2]):\n",
    "\n",
    "\n",
    "                j = 0\n",
    "                c = 0\n",
    "                if i == 0:\n",
    "                    start_idx = start_idx\n",
    "                elif num_steps % int(n[0].item()) != 0:\n",
    "                # elif i % int(n[0].item()) != 0:\n",
    "                    start_idx += step_size\n",
    "                else: \n",
    "                    start_idx += (filter_size - 1) + pad_black.shape[-2]\n",
    "                    # start_idx = int(i//int(n[0].item()) * pad_black.shape[-2])\n",
    "                    \n",
    "\n",
    "                \n",
    "                # while j < new_kern.shape[-1] and start_idx+j < new_kern.shape[-1]:\n",
    "                while j < new_kern.shape[-1] and start_idx+j+filter_size <= new_kern.shape[-1]:\n",
    "                    # print(start_idx + j)\n",
    "                    new_kern[k][i][start_idx + j:start_idx + j + filter_size] = flat_kern[k][c:c + filter_size]\n",
    "                    c += filter_size\n",
    "                    j += pad_black.shape[-2]\n",
    "                    if c == flat_kern.shape[-1]:\n",
    "                        break\n",
    "                \n",
    "                num_steps += 1\n",
    "                \n",
    "        return new_kern, flat_pad_black, n, test_pad_0, test_pad_1\n",
    "    \n",
    "    def convolve(self):\n",
    "    \n",
    "        start_time = time.time()\n",
    "\n",
    "        changed_kern, flat_pad_black, n, test_pad_0, test_pad_1 = self.change_kern_multi()\n",
    "        print(\"changed_kern = \",changed_kern.shape)\n",
    "\n",
    "        copy_flat_img = torch.cat([flat_pad_black, flat_pad_black], dim=0)\n",
    "        print(copy_flat_img.shape)\n",
    "\n",
    "        conved_mat = changed_kern @ copy_flat_img.unsqueeze(dim=-1) \n",
    "\n",
    "        print(conved_mat.shape)\n",
    "\n",
    "        squeezed_convd_mat = conved_mat.squeeze(dim=-1)\n",
    "        \n",
    "        mat_conv_test = torch.zeros(squeezed_convd_mat.shape, device=device)\n",
    "        print(\"squeezed_convd_mat shape = \",squeezed_convd_mat.shape)\n",
    "\n",
    "        # shifting back the shifted pixels because of the padding\n",
    "        # mat_conv_test[:, :mat_conv_test.shape[-1] - (test_pad_0 + test_pad_1 - 1)] = squeezed_convd_mat[:, (test_pad_0 + test_pad_1 - 1):]\n",
    "        # mat_conv_test[:, mat_conv_test.shape[-1] - (test_pad_0 + test_pad_1 - 1):] = squeezed_convd_mat[:, 0:(test_pad_0 + test_pad_1 - 1)]\n",
    "\n",
    "        # conv_img = mat_conv_test.reshape(num_features * self.img.shape[-3], int(n[-2].item()), int(n[-1].item()))\n",
    "        conv_img = squeezed_convd_mat.reshape(num_features * self.img.shape[-3], int(n[-2].item()), int(n[-1].item()))\n",
    "        end_time = time.time()\n",
    "        print(end_time - start_time)\n",
    "        return conv_img, n, changed_kern #returns convolved image \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
